\documentclass{article}
\usepackage[utf8]{amsmath}
\usepackage{flexisym}

\begin{document}

These are my notes on Section 2.7 from Abbott. I will provide proofs to various theorems on infinite series as well as provide my responses to any problems of interest. \\

(a) Prove the alternating series test by showing that $(a_n)$ is a Cauchy sequence. \\

Since we are looking to show the Cauchy Criterion, let us look at the sequence of partial sums. 

\begin{equation}
    |s_n - s_m| = |a_{m+1} + a_{m+2} + ... + a_n|
\end{equation}

Since the sign of each term is alternating by our hypothesis, and $(a_n)$ is decreasing, we can assert that 

\begin{equation}
    |s_n - s_m| = |a_{m+1} + a_{m+2} + ... + a_n| < |a_{m+1}|
\end{equation}

Since we know that $a_n \rightarrow 0$, for all n $\ge$ N, $|a_n| < \epsilon$. Therefore, for all $n, m \ge N$,

\begin{equation}
    |s_n - s_m| < \epsilon
\end{equation}

The sequence of partial sums is Cauchy, thus the series is convergent. \\

(b) Supply another proof using the Nested Interval Property. \\

Construct nested intervals in the following manner: $I_n = [s_n, s_n+1]$. By our hypothesis, we know that these intervals are nested. In addition, since $(a_n) \rightarrow 0$, for all $n \ge N$, the length of every interval $I_n$ is less than some $\epsilon$. By the Nested Interval Property, ${\bigcap}_{n=1}^\infty I_n \neq \emptyset$. Thus, we can propose some x that is contained in every $I_n$ to be the limit point of the series, since every $s_n$ for all $n \ge N$ becomes arbitrarily close to this limit point. \\

(c) Consider the subsequences $(s_{2n})$ and $(s_{2n+1})$ and show how the Monotone Convergence Theorem leads to a third proof. \\

Take note that the partial sums $(s_{2n})$ and $(s_{2n+1})$ end in even and odd values respectively. Without loss of generality, let us assume that $a_n > 0$. We can then assert that every even term in the series is less than zero, and every odd term is greater than zero. We will now show that the sequence of partial sums $(s_{2n})$ and $(s_{2n+1})$ are monotone increasing and decreasing respectively. Let us take a look at $s_{2n+2}$ and $s_{2n}$. 

\begin{equation}
    s_{2n+2} - s_{2n} = a_{2n+1} + a_{2n+2} > 0
\end{equation}

Since $a_{2n+1} > 0$, $(a_n)$ is decreasing, and $a_{2n+2} < 0$, we have shown that the sequence $(s_{2n})$ is monotone increasing. It is very similar to show that $(s_{2n+1})$ is monotone decreasing. In addition, each partial sum $s_{2n}$ is bounded above by $|s_{2n-1}|$. Similarly, $s_{2n+1}$ is bounded below by $s_{2n}$. By the Monotone Convergence Theorem, both sequences converge to limits $L_1$ and $L_2$ respectively. In addition, since $(a_n) \rightarrow 0$, 

\begin{equation}
    |s_{2n} - s_{2n+1}| < \epsilon / 3
\end{equation}

If we could show that $(s_{2n})$ and $(s_{2n+1})$ converged to the same limit, we will have shown that the series itself converges. Putting this all together yields 

\begin{equation}
\begin{split}
    |L_1 - L_2| & = | (L_1 - s_{2n}) + (s_{2n} - s_{2n+1}) + (s_{2n+1} - L_2) | \\ 
    & \leq |L_1 - s_{2n}| + |s_{2n} - s_{2n+1}| + |s_{2n+1} - L_2| \\
    & = \epsilon / 3 +  \epsilon / 3 +  \epsilon / 3  \\
    & = \epsilon
\end{split}
\end{equation}

Let us say that for $(s_{2n})$, all terms are in within $\epsilon$ of $L_1$ for all $n \ge N_1$, and for $(s_{2n+1})$, all terms are within $\epsilon$ of $L_2$ for all $n \ge N_2$. Again, say that $|a_n| < \epsilon$ for all $n \ge N_3$. Setting $N = \max \{\,N_1, N_2, N_3\}\ $satisfies the above equation. Thus, merging these two subsequences back together yields a sequence of partial sums that converge, and the statement is proven. \\

I will now give my attempt to prove a theorem from Section 2.4. \\

$Theorem: \sum_{n=1}^{\infty} 1/n^p$ converges if and only if $p > 1$. \\

To prove one direction, we will use the contrapositive. Assume $p \ge 1$. We will now show that the sequence $\sum_{n=1}^{\infty} 1/n^p$ diverges. Thankfully, we know that the harmonic series diverges, thus we can use the Comparison Test. Since $p \ge 1$, $ 1/n^p \ge 1/n$ for all $n \in \mathbb{N}$. Thus, by the Comparison Test, $\sum_{n=1}^{\infty} 1/n^p$ diverges. \\

To prove the reverse direction, we must use some facts about geometric series. Assume $p > 1$. Recall the Cauchy Condensation Test. If $\sum_{n=0}^{\infty} 2^n/(2^n)^p$ converges, then $\sum_{n=1}^{\infty} 1/n^p$ converges as well. With some simplification, the series becomes

\begin{equation}
    \sum_{n=0}^{\infty} \frac{2^n}{(2^n)^p} = \sum_{n=0}^{\infty} \frac{1}{(2^n)^{p-1}} = \sum_{n=0}^{\infty} (\frac{1}{2^{p-1}})^n
\end{equation}

Note that this is a geometric series. Since $p > 1$, $1/2^{p-1} < 1$. Thus, this is a geometric series, and the series $\sum_{n=1}^{\infty} 1/n^p$ converges. 

\newpage

Various tests for the convergence of series are given. I will provide my attempts at proofs of their correctness here. \\

$Thereom:$ Given a series $\sum_{n=1}^{\infty} a_n$ with $a_n \neq 0$, the Ratio Test states that if $(a_n)$ satisfies 

\begin{equation}
    \lim_{n\to\infty} \bigg|\frac{a_{n+1}}{a_n}\bigg| = r < 1
\end{equation}

then the series converges absolutely. \\

Let $a\textprime$ satisfy $r < r\textprime < 1$. We can guarantee such a $r\textprime$ exists by the density of $\mathbb{Q}$ in $\mathbb{R}$. Also, by our definition of convergence, we can rewrite the above limit, arguing that for all $n \geq N$, 

\begin{equation}
    \bigg|\bigg|\frac{a_{n+1}}{a_n}\bigg| - r\bigg| < \epsilon
\end{equation}


We can rewrite this inequality as

\begin{equation}
    r - \epsilon <\bigg|\frac{a_{n+1}}{a_n}\bigg| < \epsilon + r
\end{equation}


Multiplying each side of the inequality by $|a_n|$ yields

\begin{equation}
    |a_n|(r-\epsilon) < |a_{n+1}| < |a_n|(r+\epsilon)
\end{equation}

Fix $\epsilon$ to be small enough so that $r+\epsilon < r\textprime$. It follows that 

\begin{equation}
    |a_{n+1}| < |a_n|r\textprime
\end{equation}

Note that this holds for all $n \geq N$. Now, consider the series $|a_N|\sum (r\textprime)^n$. Because $r\textprime < 1$, this is a geometric series, which we have already shown converges. Now, recall the above inequality. 

\begin{equation}
\begin{split}
    & |a_{N+1}| < |a_N|r\textprime \\
    & |a_{N+2}| < |a_N|(r\textprime)^2 \\
    & |a_{N+3}| < |a_N|(r\textprime)^3 \\
    & \hspace{11 mm} \vdots
\end{split}
\end{equation}

With this inductive argument, the Comparison Test implies that $\sum |a_n|$ converges. \\

\newpage

Below is a nice tool for working with series. \\

Let $(x_n)$ and $(y_n)$ be sequences, and let $s_n = x_1 + x_2 + \dots + x_n$. Use the observation that $x_j = s_j - s_{j-1}$ to verify the formula

\begin{equation}
    \sum_{j=m+1}^{n} x_j y_j = s_n y_{n+1} - s_m y_{m+1} + \sum_{j=m+1}^{n} s_j(y_{j} - y_{j+1})
\end{equation}

Let us expand out the summation $\sum_{j=m+1}^{n} s_j(y_{j+1} - y_j)$ from the right side of this formula. 

\begin{equation}
\begin{split}
    & \hspace{5 mm} s_{m+1}(y_{m+1} - y_{m+2}) + s_{m+2}(y_{m+2} - y_{m+3}) + \dots + s_{n}(y_{n} - y_{n+1}) \\[15pt]
    & = s_{m+1}y_{m+1} + y_{m+2}(s_{m+2} - s_{m+1}) + \dots + y_n(s_{n} - s_{n-1}) - s_n y_{n+1} \\[15pt]
    & = s_{m+1}y_{m+1} + y_{m+2}x_{m+2} + \dots + y_n x_n - s_n y_{n+1} \\[10pt]
    & = s_{m+1}y_{m+1} - s_n y_{n+1} + \sum_{j=m+2}^{n} x_j y_j
\end{split}
\end{equation}

Adding this back to the rest of the right-hand side of the equation yields

\begin{equation}
\begin{split}
    & \hspace{5 mm} s_n y_{n+1} - s_m y_{m+1} + s_{m+1}y_{m+1} - s_n y_{n+1} + \sum_{j=m+2}^{n} x_j y_j \\
    &= y_{m+1}(s_{m+1} - s_m) + ( s_n y_{n+1} - s_n y_{n+1}) + \sum_{j=m+2}^{n} x_j y_j \\
    &= x_{m+1} y_{m+1} + \sum_{j=m+2}^{n} x_j y_j \\
    &= \sum_{j=m+1}^{n} x_j y_j
\end{split}
\end{equation}

Thus, the right-hand side equals the left-hand side as desired. This lemma will come in handy for the next proof. 
\newpage

Dirichlet's Test for convergence states that if the partial sums of $\sum_{n=1}^{\infty} x_n$ are bounded (but not necessarily convergent),
and if $(y_n)$ is a sequence satisfying $y_1 \geq y_2 \geq y_3 \geq \dots \geq 0$ with $\lim_{n\to \infty} y_n = 0$,
then the series $\sum_{n=1}^{\infty} x_n y_n$ converges. \\

Let $M > 0$ be a bound on the partial sums of $\sum_{n=1}^{\infty} x_n$. Then, 

\begin{equation}
\begin{split}
    &\hspace{-20 mm}\Bigg|\sum_{j=m+1}^{n} x_j y_j \Bigg| = \Bigg| s_n y_{n+1} - s_m y_{m+1} + \sum_{j=m+1}^{n} s_j(y_{j} - y_{j+1})\Bigg| \\
    &\leq | M y_{n+1} - s_m y_{m+1} + \sum_{j=m+1}^{n} M(y_{j} - y_{j+1}) | \\
    &= | M y_{n+1} - s_m y_{m+1} + M y_{m+1} - M y_{n+1} | \\[10 pt]
    &\leq | (M - s_m) y_{m+1} |
\end{split}
\end{equation}

Since $M$ is a bound on the partial sums $s_n$, $M - s_m$ is at most $2M$. This gives us the desired result

\begin{equation}
    \Bigg|\sum_{j=m+1}^{n} x_j y_j \Bigg| \leq 2M|y_{m+1}|
\end{equation}

Since $y_n \rightarrow 0$, we can argue that for all $m \geq N$, 

\begin{equation}
    | y_m | < \frac{\epsilon}{2M}
\end{equation}

Notice how in (18), the right-hand side is independent of the choice of n. Thus, the inequality becomes

\begin{equation}
\begin{split}
    &\Bigg|\sum_{j=m+1}^{n} x_j y_j \Bigg| \leq 2M(\frac{\epsilon}{2M}) = \epsilon
\end{split}
\end{equation}

Let $S_n$ represent the $n^{th}$ partial sum of the entire series. Thus, we have shown that for all $n, m \geq N$, $|S_n - S_m| < \epsilon$. Therefore, the sequence of partial sums is Cauchy, and the series converges. \\

Notice how the Alternating Series Test is a special case of Dirichlet's Test. Consider a monotone decreasing sequence $(y_n)$ that converges to 0, just as in the hypothesis of the theorem. Then, consider the sequence $x_n = (-1)^n$. The product of these two series satisfies the hypothesis of the Alternating Series Test, and by Dirichlet's Test is a convergent series. 

\newpage

Lastly, I will conclude with my proof of Abel's Test. Abel's Test for convergence states that if the series $\sum_{n=1}^\infty x_n$ converges, and if $(y_n)$ is a sequence satisfying 

\begin{equation}
    y_1 \leq y_2 \leq y_3 \leq \dots 0
\end{equation}

then the series $\sum_{n=1}^\infty x_n y_n$ converges. \\

First, note how the hypothesis of Abel's test differs from that of Dirichlet's Test. The hypothesis of Dirichlet's Test requires that the sequence $y_n \rightarrow 0$, unlike Abel's Test. In addition, in Abel's Test, the series $\sum_{n=1}^\infty x_n$ must converge, whereas in Dirichlet's Test this condition need not be met. \\

Assume that $\sum_{n=1}^\infty a_n$ has partial sums that are bounded by a constant $A > 0$, and assume $b_1 \geq b_2 \geq b_3 \geq \dots 0$. 

Without repeating the work from the last proof, we can assert the follow inequality. 

\begin{equation}
    \Bigg|\sum_{j=1}^n a_j b_j\Bigg| \leq 2Ab_1
\end{equation}

With this inequality, and a bit of substitution, we will arrive at the conclusion that the series in question is Cauchy, and is thus convergent. Firstly, note that since $\sum_{n=1}^\infty x_n$ is convergent, the sequence of partial sums is Cauchy. Thus, for all $n, m \geq N$,  

\begin{equation}
    \sum_{j=m+1}^n x_j = |s_n - s_m | < \frac{\epsilon}{4y_{m+1}}
\end{equation}

Thus, for sufficiently large $m$, $\epsilon / 2y_{m+1}$ is a bound on the partial sums of $\sum_{j=m+1}^n x_j$. Now, for a fixed $m \in N$, set $a_n = x_{m+n}$ and $b_n = y_{m+n}$. With this substitution, the inequality becomes 

\begin{equation}
    \Bigg|\sum_{j=m+1}^n x_j x_j\Bigg| = \Bigg|\sum_{j=1}^{n-m} a_j b_j\Bigg| \leq 2Ab_1
\end{equation}

Now, note that in this situation, $b_1 = y_m+1$. In addition, $A$ is an upper bound on the partial sums of $a_n$. We have already shown, however, that for sufficiently large $m$, the partial sums of $\sum_{j=m+1}^n x_j$ are bounded above by $\epsilon / 2y_{m+1}$. Thus, the inequality becomes 

\begin{equation}
    \Bigg|\sum_{j=m+1}^n x_j y_j\Bigg| \leq 2(y_{m+1})(\frac{\epsilon}{4y_{m+1}}) = \epsilon / 2 < \epsilon
\end{equation}

Let $S_n$ represent the $n^{th}$ partial sum of $\sum_{n=1}^\infty x_n y_n$. Thus, for all $n, m \geq N$, 

\begin{equation}
    |S_n - S_m| =  \Bigg|\sum_{j=m+1}^n x_j y_j\Bigg| < \epsilon
\end{equation}

and the sequence is Cauchy. Thus, the series is convergent, and the theorem is proven. \\

Last Tuesday, we talked about the convergence of certain infinite series to some very interesting limits. When I looked into the topic, it seemed that most of the techniques used to prove the existence of such limits were out of the scope of my knowledge of analysis. I did, however, find out the reasoning behind why the alternating harmonic series converges to $\ln 2$. \\

Without going too deep into the existence or construction of Taylor series, let us assume for now that the terms of a Taylor series follow the following pattern. 

\begin{equation}
    f(x) = \sum_{n=0}^\infty \frac{f^{(n)}(0)}{n!} x^n
\end{equation}

Consider the function $f(x) = ln(1+x)$. The derivatives of this function can be written in terms of $n$ as

\begin{equation}
    f^{(n)}(a) = (-1)^{n+1}\frac{(n-1)!}{(1+a)^n}
\end{equation}

Substituting this back into the original function yields

\begin{equation}
    f(x) = \sum_{n=0}^\infty \frac{f^{(n)}(0)}{n!} x^n = \sum_{n=0}^\infty (-1)^{n+1}\frac{x^n}{n}
\end{equation}

It follows that

\begin{equation}
    f(1) = ln(2) = \sum_{n=1}^\infty (-1)^{n+1}\frac{1}{n} = 1 - \frac{1}{2} + \frac{1}{3} - \frac{1}{4} + \frac{1}{5} - \dots 
\end{equation}

Note that the index of the infinite sum starts from 1 rather than 0. This is because the term $a_0$ in the Taylor series is defined as $\frac{f(0)}{0!}$, and since $f(0) = 0$, we need not include the first term in our series. 

\end{document}
